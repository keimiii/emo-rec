{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Model: Detection, Cropping, and Training Prep\n",
    "\n",
    "This notebook prepares a single-face dataset from FindingEmo and sketches a lightweight model for Valence/Arousal regression.\n",
    "\n",
    "Data directory expected: `data/Run_1/`, `data/Run_2/`, with annotations in `data/processed_annotations.csv` (produced by scripts/findingemo_process_annotations.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment & Dependencies\n",
    "- Requires: opencv-python, mediapipe, numpy, pandas, torch, torchvision.\n",
    "- In this project, prefer installing from shell with: `uv pip install mediapipe torch torchvision`\n",
    "- If running directly in this notebook, you may use: `!pip install mediapipe torch torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375ca9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mediapipe torch torchvision --quiet\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "RUN_DIRS = [DATA_DIR / \"Run_1\", DATA_DIR / \"Run_2\"]\n",
    "ANNOTATIONS_CSV = DATA_DIR / \"processed_annotations.csv\"\n",
    "FACE_CROPS_DIR = DATA_DIR / \"face_crops\"\n",
    "FACE_CSV = DATA_DIR / \"face_annotations.csv\"\n",
    "FACE_CROPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert ANNOTATIONS_CSV.exists(), f\"Expected annotations at {ANNOTATIONS_CSV}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5bb349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config knobs ---\n",
    "CONFIDENCE_THRESHOLD = 0.5  # lower -> more detections (incl. false positives)\n",
    "MODEL_SELECTION = (\n",
    "    0  # 0: short-range (bigger/closer faces), 1: long-range (smaller/farther faces)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SingleFaceProcessor (MediaPipe)\n",
    "Chooses the primary face (confidence × area × center proximity), then extracts a padded crop resized to 224×224."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e22979",
   "metadata": {},
   "source": [
    "**What MediaPipe is**\n",
    "\n",
    "* MediaPipe is a Google framework that ships fast, pre-trained CV models (pose, hands, face) with easy Python APIs.  \n",
    "* Here we use `mp.solutions.face_detection.FaceDetection`, a lightweight face detector (BlazeFace family) that runs in real time on CPU.  \n",
    "\n",
    "**What the code does**\n",
    "\n",
    "* Converts the image from BGR to RGB (OpenCV loads images as BGR; models typically expect RGB).\n",
    "* Runs MediaPipe face detection to get zero or more face detections.\n",
    "* Scores each detected face with: score = confidence × sqrt(area) × (0.7 + 0.3 × center_proximity)\n",
    "* confidence: the detector’s probability for that face.\n",
    "* area: prefers larger faces (proxy for closeness/visibility).\n",
    "* center proximity: prefers faces near the image center.\n",
    "* Selects the best-scoring face.\n",
    "* Extracts a padded crop around that face and resizes to 224×224.\n",
    "* Returns the crop and the bounding box; if no faces, returns None, None.\n",
    "\n",
    "**Why resize to 224×224?**\n",
    "\n",
    "* It’s a convention for many ImageNet models (e.g., ResNet18) which expect 224×224 inputs.  \n",
    "* Using 224×224 gives you compatibility with torchvision backbones and pretraining. You could pick other sizes, but then you’d adjust the model or transforms accordingly.  \n",
    "\n",
    "**What is OpenCV (cv2) and how it’s used here**\n",
    "\n",
    "* OpenCV is a widely used computer-vision library.\n",
    "* `cv2.cvtColor(image, cv2.COLOR_BGR2RGB)`: convert BGR→RGB for the detector.\n",
    "* `cv2.resize(crop, (224, 224))`: resize the face crop to the target size.\n",
    "* You’ll also typically use cv2.imread to load images and cv2.imwrite to save crops elsewhere in the notebook.  \n",
    "\n",
    "**About confidence_threshold**\n",
    "\n",
    "* FaceDetection(min_detection_confidence=confidence_threshold) tells MediaPipe to only return detections with confidence ≥ that threshold.\n",
    "* In extract_primary_face, if results.detections is empty (e.g., no face ≥ threshold), it returns None, None. So yes, with too-high thresholds you may get no output.\n",
    "* If you lower the threshold (e.g., 0.3), you’ll get more candidate faces (including some false positives). If you raise it (e.g., 0.8), you’ll get fewer but higher-confidence detections.\n",
    "* Practical tip: start around 0.5, then adjust based on how many frames come back empty vs. how many false crops you see.  \n",
    "\n",
    "**Crop padding and safety**\n",
    "\n",
    "* The crop uses a 20% padding around the detected box and clamps to image bounds.\n",
    "* If the crop is empty (edge case), it returns None to avoid downstream errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleFaceProcessor:\n",
    "    def __init__(self, confidence_threshold: float = 0.5, model_selection: int = 0):\n",
    "        \"\"\"\n",
    "        MediaPipe single-face selector:\n",
    "        - confidence_threshold: min detection confidence (0..1)\n",
    "        - model_selection: 0 = short-range model, 1 = long-range model\n",
    "        \"\"\"\n",
    "        self.mp_face = mp.solutions.face_detection\n",
    "        self.detector = self.mp_face.FaceDetection(\n",
    "            min_detection_confidence=confidence_threshold,\n",
    "            model_selection=model_selection,\n",
    "        )\n",
    "\n",
    "    def extract_primary_face(self, image: np.ndarray):\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = self.detector.process(rgb)\n",
    "        if not results.detections:\n",
    "            return None, None\n",
    "        bbox = self._select_best_face(results.detections, image.shape)\n",
    "        if bbox is None:\n",
    "            return None, None\n",
    "        crop = self._extract_face_crop(image, bbox)\n",
    "        return crop, bbox\n",
    "\n",
    "    def _select_best_face(self, detections, image_shape):\n",
    "        h, w = image_shape[:2]\n",
    "        center = np.array([w / 2, h / 2])\n",
    "        best, best_score = None, -1.0\n",
    "        for det in detections:\n",
    "            bb = det.location_data.relative_bounding_box\n",
    "            conf = det.score[0]\n",
    "            area = max(bb.width, 0) * max(bb.height, 0)\n",
    "            face_center = np.array(\n",
    "                [bb.xmin + bb.width / 2, bb.ymin + bb.height / 2]\n",
    "            ) * np.array([w, h])\n",
    "            dist = np.linalg.norm(face_center - center)\n",
    "            maxd = np.linalg.norm(center) or 1.0\n",
    "            prox = 1 - (dist / maxd)\n",
    "            score = float(conf) * float(np.sqrt(area)) * (0.7 + 0.3 * float(prox))\n",
    "            if score > best_score:\n",
    "                best, best_score = bb, score\n",
    "        return best\n",
    "\n",
    "    def _extract_face_crop(self, image: np.ndarray, bb, padding: float = 0.2):\n",
    "        h, w = image.shape[:2]\n",
    "        x0 = int(max(0, (bb.xmin - padding * bb.width) * w))\n",
    "        y0 = int(max(0, (bb.ymin - padding * bb.height) * h))\n",
    "        x1 = int(min(w, (bb.xmin + bb.width * (1 + padding)) * w))\n",
    "        y1 = int(min(h, (bb.ymin + bb.height * (1 + padding)) * h))\n",
    "        crop = image[y0:y1, x0:x1]\n",
    "        if crop.size == 0:\n",
    "            return None\n",
    "        return cv2.resize(crop, (224, 224), interpolation=cv2.INTER_AREA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dry-run detection stats (no writes) ---\n",
    "# Measures: total rows/images in annotations, how many exist locally, how many get a detection\n",
    "\n",
    "df_all = pd.read_csv(ANNOTATIONS_CSV)\n",
    "df_all[\"local_path\"] = df_all[\"image_path\"].apply(\n",
    "    lambda p: str(DATA_DIR / p.lstrip(\"/\"))\n",
    ")\n",
    "df_all[\"exists\"] = df_all[\"local_path\"].apply(lambda p: Path(p).exists())\n",
    "\n",
    "total_rows = len(df_all)\n",
    "total_images = df_all[\"image_path\"].nunique()\n",
    "exists_rows = int(df_all[\"exists\"].sum())\n",
    "exists_images = df_all[df_all[\"exists\"]][\"image_path\"].nunique()\n",
    "\n",
    "processor = SingleFaceProcessor(\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD, model_selection=MODEL_SELECTION\n",
    ")\n",
    "\n",
    "det_rows = 0\n",
    "det_images = set()\n",
    "\n",
    "for _, row in tqdm(df_all[df_all[\"exists\"]].iterrows(), total=exists_rows):\n",
    "    img = cv2.imread(row[\"local_path\"])\n",
    "    if img is None:\n",
    "        continue\n",
    "    crop, bb = processor.extract_primary_face(img)\n",
    "    if crop is None:\n",
    "        continue\n",
    "    det_rows += 1\n",
    "    det_images.add(row[\"image_path\"])\n",
    "\n",
    "stats = {\n",
    "    \"total_rows_in_annotations\": total_rows,\n",
    "    \"total_unique_images_in_annotations\": total_images,\n",
    "    \"rows_with_local_file\": exists_rows,\n",
    "    \"unique_images_with_local_file\": exists_images,\n",
    "    \"detected_rows\": det_rows,\n",
    "    \"detected_unique_images\": len(det_images),\n",
    "    \"confidence_threshold\": CONFIDENCE_THRESHOLD,\n",
    "    \"model_selection\": MODEL_SELECTION,\n",
    "}\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build face crops + metadata (combine Run_1 and Run_2)\n",
    "Input annotations: `data/processed_annotations.csv` with columns like `image_path`, `valence`, `arousal`, `emotion`, `user`, etc.  \n",
    "\n",
    "Output: `data/face_crops/` directory and `data/face_annotations.csv` with one row per successful face crop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1754918801.572535  807678 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1754918801.583924  809652 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "  1%|          | 200/20147 [00:01<02:26, 135.74it/s]Premature end of JPEG file\n",
      "  1%|          | 214/20147 [00:01<02:27, 135.24it/s][ERROR:0@117.619] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Appalled teenagers soccer/UAT4VQJ7VLN3UWZ7W2YNJWTO2I.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      "  2%|▏         | 327/20147 [00:02<02:21, 140.05it/s]Premature end of JPEG file\n",
      "  2%|▏         | 389/20147 [00:02<02:19, 141.49it/s]Premature end of JPEG file\n",
      "  3%|▎         | 508/20147 [00:03<02:13, 147.57it/s]Premature end of JPEG file\n",
      "  3%|▎         | 608/20147 [00:04<02:12, 147.95it/s]Premature end of JPEG file\n",
      "  4%|▎         | 732/20147 [00:05<01:55, 168.58it/s]Premature end of JPEG file\n",
      "  6%|▌         | 1148/20147 [00:08<03:17, 96.22it/s] [ERROR:0@124.748] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Mocking fifty-something sports/jared-goff-ncaa-football-armed-forces-bowl-california-vs-air-force.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      "  7%|▋         | 1449/20147 [00:10<02:25, 128.57it/s]Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      "  8%|▊         | 1530/20147 [00:11<02:31, 122.95it/s][ERROR:0@127.629] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Offended thirty-something spring/DWWHHKZP3FC7NJ27EOMSC425NA.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 10%|▉         | 1932/20147 [00:14<01:58, 153.77it/s]Premature end of JPEG file\n",
      " 11%|█         | 2117/20147 [00:15<02:24, 125.18it/s]Premature end of JPEG file\n",
      " 11%|█         | 2197/20147 [00:16<01:57, 152.57it/s]Premature end of JPEG file\n",
      " 12%|█▏        | 2383/20147 [00:17<02:03, 143.77it/s][ERROR:0@133.629] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Outraged forty-something soccer/545403818-annual-allen-and-co-investors-meeting-draws-ceos-and-business-leaders-to-sun-valley-idaho.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 12%|█▏        | 2413/20147 [00:17<02:04, 142.80it/s]Premature end of JPEG file\n",
      " 12%|█▏        | 2471/20147 [00:18<02:27, 120.21it/s]Premature end of JPEG file\n",
      " 13%|█▎        | 2589/20147 [00:19<02:15, 129.35it/s]Premature end of JPEG file\n",
      " 13%|█▎        | 2636/20147 [00:19<02:04, 140.98it/s]Premature end of JPEG file\n",
      " 16%|█▌        | 3188/20147 [00:23<02:38, 106.82it/s]Premature end of JPEG file\n",
      " 17%|█▋        | 3344/20147 [00:24<01:58, 142.24it/s][ERROR:0@141.120] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Irritated fifty-something home/899643200-milwaukee-bucks-v-oklahoma-city-thunder.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 17%|█▋        | 3491/20147 [00:26<02:14, 123.53it/s]Premature end of JPEG file\n",
      " 21%|██▏       | 4326/20147 [00:32<01:51, 142.34it/s]Premature end of JPEG file\n",
      " 22%|██▏       | 4355/20147 [00:32<01:57, 134.69it/s]Premature end of JPEG file\n",
      " 23%|██▎       | 4576/20147 [00:34<01:45, 147.98it/s][ERROR:0@150.470] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Embarrassed soldiers sports/9647629-nfl-minnesota-vikings-chicago-bears-850x560.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 23%|██▎       | 4591/20147 [00:34<01:48, 143.51it/s]Premature end of JPEG file\n",
      " 23%|██▎       | 4676/20147 [00:34<01:36, 160.24it/s][ERROR:0@151.109] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Envy teenagers spring/487638154-nikelevis-kids-rock-runway-show.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 31%|███▏      | 6311/20147 [00:47<01:29, 155.17it/s]libpng warning: iCCP: known incorrect sRGB profile\n",
      " 32%|███▏      | 6360/20147 [00:47<01:34, 146.03it/s]Premature end of JPEG file\n",
      " 33%|███▎      | 6588/20147 [00:49<01:34, 143.77it/s]Premature end of JPEG file\n",
      " 36%|███▌      | 7222/20147 [00:54<01:32, 139.75it/s]Premature end of JPEG file\n",
      " 39%|███▊      | 7786/20147 [00:58<01:35, 129.21it/s]Premature end of JPEG file\n",
      " 40%|████      | 8071/20147 [01:00<01:16, 157.36it/s]Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      " 41%|████      | 8173/20147 [01:01<01:31, 131.02it/s]Premature end of JPEG file\n",
      " 41%|████      | 8190/20147 [01:01<01:26, 138.59it/s][ERROR:0@177.435] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Bitter thirty-something shop/75667-dreamworks.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 43%|████▎     | 8727/20147 [01:05<01:13, 155.62it/s]Premature end of JPEG file\n",
      " 44%|████▍     | 8866/20147 [01:06<01:37, 115.81it/s]Premature end of JPEG file\n",
      " 44%|████▍     | 8959/20147 [01:07<01:21, 137.43it/s]Premature end of JPEG file\n",
      " 47%|████▋     | 9509/20147 [01:11<01:21, 131.12it/s][ERROR:0@187.326] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Embarrassed thirty-something soccer/59562d88e584e630b4000001.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 48%|████▊     | 9701/20147 [01:12<01:12, 144.84it/s]Premature end of JPEG file\n",
      " 48%|████▊     | 9760/20147 [01:13<01:23, 123.85it/s]Premature end of JPEG file\n",
      " 49%|████▊     | 9810/20147 [01:13<01:42, 100.87it/s]Premature end of JPEG file\n",
      " 49%|████▉     | 9949/20147 [01:14<01:26, 118.55it/s][ERROR:0@190.937] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Shocked retiree summer/Michael-Jordan-1994-.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 52%|█████▏    | 10515/20147 [01:18<01:32, 104.64it/s]Premature end of JPEG file\n",
      " 53%|█████▎    | 10595/20147 [01:19<01:03, 150.10it/s]libpng error: Read Error\n",
      " 54%|█████▍    | 10928/20147 [01:21<01:11, 128.88it/s]Premature end of JPEG file\n",
      " 55%|█████▍    | 10992/20147 [01:22<01:02, 146.99it/s]Premature end of JPEG file\n",
      " 57%|█████▋    | 11554/20147 [01:26<01:14, 115.61it/s]Premature end of JPEG file\n",
      " 59%|█████▊    | 11827/20147 [01:28<00:57, 145.54it/s]Premature end of JPEG file\n",
      "Premature end of JPEG file\n",
      " 61%|██████    | 12236/20147 [01:31<01:02, 125.87it/s]Premature end of JPEG file\n",
      " 64%|██████▍   | 12905/20147 [01:37<01:10, 103.39it/s]Premature end of JPEG file\n",
      " 65%|██████▍   | 13012/20147 [01:38<01:00, 117.67it/s]Premature end of JPEG file\n",
      " 66%|██████▌   | 13275/20147 [01:40<00:50, 135.85it/s]Premature end of JPEG file\n",
      " 66%|██████▌   | 13323/20147 [01:41<00:48, 139.31it/s]Premature end of JPEG file\n",
      " 66%|██████▋   | 13352/20147 [01:41<00:56, 120.71it/s]Premature end of JPEG file\n",
      " 67%|██████▋   | 13483/20147 [01:42<00:55, 121.12it/s]Premature end of JPEG file\n",
      " 67%|██████▋   | 13594/20147 [01:43<00:44, 147.37it/s]Premature end of JPEG file\n",
      " 69%|██████▊   | 13803/20147 [01:44<00:46, 135.13it/s]Premature end of JPEG file\n",
      " 70%|██████▉   | 14095/20147 [01:46<00:39, 154.28it/s]Premature end of JPEG file\n",
      " 71%|███████   | 14227/20147 [01:47<00:37, 159.08it/s]Premature end of JPEG file\n",
      " 71%|███████   | 14243/20147 [01:47<00:43, 136.93it/s]Premature end of JPEG file\n",
      " 72%|███████▏  | 14414/20147 [01:49<00:37, 154.31it/s]Premature end of JPEG file\n",
      " 72%|███████▏  | 14430/20147 [01:49<00:40, 142.62it/s]Premature end of JPEG file\n",
      " 72%|███████▏  | 14484/20147 [01:49<00:48, 116.57it/s]Premature end of JPEG file\n",
      " 74%|███████▎  | 14851/20147 [01:52<00:39, 132.68it/s]Premature end of JPEG file\n",
      " 75%|███████▍  | 15073/20147 [01:54<00:38, 131.64it/s]Premature end of JPEG file\n",
      " 78%|███████▊  | 15664/20147 [01:58<00:34, 128.37it/s]Premature end of JPEG file\n",
      " 78%|███████▊  | 15742/20147 [01:59<00:37, 117.60it/s]Premature end of JPEG file\n",
      " 79%|███████▊  | 15826/20147 [02:00<00:37, 115.45it/s]Premature end of JPEG file\n",
      " 82%|████████▏ | 16466/20147 [02:05<00:28, 127.01it/s]Premature end of JPEG file\n",
      " 82%|████████▏ | 16511/20147 [02:05<00:32, 111.37it/s]Premature end of JPEG file\n",
      " 82%|████████▏ | 16552/20147 [02:06<00:29, 122.58it/s][ERROR:0@242.248] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Suffering twenty-something soccer/crystal-palace-v-liverpool-fc-premier-league-5de0e25629da787ebf000001.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 84%|████████▍ | 16980/20147 [02:09<00:20, 156.17it/s]Premature end of JPEG file\n",
      " 85%|████████▌ | 17217/20147 [02:11<00:28, 104.16it/s]Premature end of JPEG file\n",
      " 86%|████████▌ | 17232/20147 [02:11<00:25, 115.45it/s]Premature end of JPEG file\n",
      " 86%|████████▌ | 17367/20147 [02:12<00:19, 145.00it/s]Premature end of JPEG file\n",
      " 88%|████████▊ | 17673/20147 [02:14<00:18, 136.55it/s]Premature end of JPEG file\n",
      " 88%|████████▊ | 17784/20147 [02:15<00:16, 140.96it/s]Premature end of JPEG file\n",
      " 89%|████████▉ | 17991/20147 [02:16<00:16, 129.02it/s]Premature end of JPEG file\n",
      " 89%|████████▉ | 18011/20147 [02:17<00:14, 145.65it/s]Premature end of JPEG file\n",
      " 94%|█████████▍| 18893/20147 [02:23<00:09, 137.72it/s]Premature end of JPEG file\n",
      " 94%|█████████▍| 18962/20147 [02:24<00:08, 144.02it/s]Premature end of JPEG file\n",
      " 95%|█████████▍| 19060/20147 [02:24<00:07, 148.47it/s]Premature end of JPEG file\n",
      " 95%|█████████▌| 19166/20147 [02:25<00:07, 129.60it/s]Premature end of JPEG file\n",
      " 96%|█████████▋| 19441/20147 [02:27<00:04, 146.03it/s][ERROR:0@263.787] global loadsave.cpp:507 imread_ imread_('../data/Run_2/Frustrated people party/7e91365b-95dc-4d0d-963b-37c06938885b-USP_NCAA_Basketball__Final_Four-Semifinals-North_C_7.jpg'): can't read data: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/grfmt_jpeg2000_openjpeg.cpp:645: error: (-2:Unspecified error) in function 'virtual bool cv::detail::Jpeg2KOpjDecoderBase::readData(Mat &)'\n",
      "> OpenJPEG2000: tiles are not supported (expected: '(int)comp.dx == 1'), where\n",
      ">     '(int)comp.dx' is 2\n",
      "> must be equal to\n",
      ">     '1' is 1\n",
      "\n",
      " 97%|█████████▋| 19565/20147 [02:28<00:04, 142.94it/s]Premature end of JPEG file\n",
      " 99%|█████████▊| 19850/20147 [02:30<00:02, 130.35it/s]Premature end of JPEG file\n",
      " 99%|█████████▉| 19946/20147 [02:31<00:01, 156.11it/s]Premature end of JPEG file\n",
      " 99%|█████████▉| 19997/20147 [02:31<00:01, 149.40it/s]Premature end of JPEG file\n",
      "100%|██████████| 20147/20147 [02:32<00:00, 131.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crop_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>run</th>\n",
       "      <th>emotion</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>user</th>\n",
       "      <th>bbox_xmin</th>\n",
       "      <th>bbox_ymin</th>\n",
       "      <th>bbox_w</th>\n",
       "      <th>bbox_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/face_crops/Run_2/Frustrated forty-some...</td>\n",
       "      <td>/Run_2/Frustrated forty-something office/team-...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Interest</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5fd97d5b40332e276ea58209</td>\n",
       "      <td>0.487954</td>\n",
       "      <td>0.197185</td>\n",
       "      <td>0.209047</td>\n",
       "      <td>0.282760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/face_crops/Run_2/Remorseful toddlers c...</td>\n",
       "      <td>/Run_2/Remorseful toddlers court of law/dcfs-c...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Interest</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5985f6bdeef500000111db98</td>\n",
       "      <td>0.698446</td>\n",
       "      <td>0.256576</td>\n",
       "      <td>0.198112</td>\n",
       "      <td>0.297020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/face_crops/Run_2/Scared adolescents pr...</td>\n",
       "      <td>/Run_2/Scared adolescents prison/15-hampton-ro...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Anger</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5fd97d5b40332e276ea58209</td>\n",
       "      <td>0.621183</td>\n",
       "      <td>0.261322</td>\n",
       "      <td>0.193928</td>\n",
       "      <td>0.344931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/face_crops/Run_2/Cheerful soldiers des...</td>\n",
       "      <td>/Run_2/Cheerful soldiers desert/obamaslides_01...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Joy</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5985f6bdeef500000111db98</td>\n",
       "      <td>0.668780</td>\n",
       "      <td>0.132012</td>\n",
       "      <td>0.195301</td>\n",
       "      <td>0.282740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/face_crops/Run_2/Raging elderly war/vi...</td>\n",
       "      <td>/Run_2/Raging elderly war/vietvetwel0010.jpg</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Joy</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5985f6bdeef500000111db98</td>\n",
       "      <td>0.099873</td>\n",
       "      <td>0.280671</td>\n",
       "      <td>0.162464</td>\n",
       "      <td>0.230036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           crop_path  \\\n",
       "0  ../data/face_crops/Run_2/Frustrated forty-some...   \n",
       "1  ../data/face_crops/Run_2/Remorseful toddlers c...   \n",
       "2  ../data/face_crops/Run_2/Scared adolescents pr...   \n",
       "3  ../data/face_crops/Run_2/Cheerful soldiers des...   \n",
       "4  ../data/face_crops/Run_2/Raging elderly war/vi...   \n",
       "\n",
       "                                          image_path    run   emotion  \\\n",
       "0  /Run_2/Frustrated forty-something office/team-...  Run_2  Interest   \n",
       "1  /Run_2/Remorseful toddlers court of law/dcfs-c...  Run_2  Interest   \n",
       "2  /Run_2/Scared adolescents prison/15-hampton-ro...  Run_2     Anger   \n",
       "3  /Run_2/Cheerful soldiers desert/obamaslides_01...  Run_2       Joy   \n",
       "4       /Run_2/Raging elderly war/vietvetwel0010.jpg  Run_2       Joy   \n",
       "\n",
       "   valence  arousal                      user  bbox_xmin  bbox_ymin    bbox_w  \\\n",
       "0        0        2  5fd97d5b40332e276ea58209   0.487954   0.197185  0.209047   \n",
       "1        1        2  5985f6bdeef500000111db98   0.698446   0.256576  0.198112   \n",
       "2        0        3  5fd97d5b40332e276ea58209   0.621183   0.261322  0.193928   \n",
       "3        2        4  5985f6bdeef500000111db98   0.668780   0.132012  0.195301   \n",
       "4        3        4  5985f6bdeef500000111db98   0.099873   0.280671  0.162464   \n",
       "\n",
       "     bbox_h  \n",
       "0  0.282760  \n",
       "1  0.297020  \n",
       "2  0.344931  \n",
       "3  0.282740  \n",
       "4  0.230036  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(ANNOTATIONS_CSV)\n",
    "# Normalize image_path to local file path: it starts with '/Run_*/...' relative to data/\n",
    "df[\"local_path\"] = df[\"image_path\"].apply(lambda p: str(DATA_DIR / p.lstrip(\"/\")))\n",
    "\n",
    "processor = SingleFaceProcessor(\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD, model_selection=MODEL_SELECTION\n",
    ")\n",
    "records = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    img_path = Path(row[\"local_path\"])\n",
    "    if not img_path.exists():\n",
    "        continue\n",
    "    image = cv2.imread(str(img_path))\n",
    "    if image is None:\n",
    "        continue\n",
    "    crop, bb = processor.extract_primary_face(image)\n",
    "    if crop is None or bb is None:\n",
    "        continue\n",
    "    # Build crop path mirroring original structure\n",
    "    rel_under_data = img_path.relative_to(DATA_DIR)\n",
    "    crop_path = FACE_CROPS_DIR / rel_under_data\n",
    "    crop_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cv2.imwrite(str(crop_path), crop)\n",
    "\n",
    "    records.append(\n",
    "        {\n",
    "            \"crop_path\": str(crop_path),\n",
    "            \"image_path\": row[\"image_path\"],\n",
    "            \"run\": rel_under_data.parts[0] if len(rel_under_data.parts) > 0 else None,\n",
    "            \"emotion\": row.get(\"emotion\"),\n",
    "            \"valence\": row.get(\"valence\"),\n",
    "            \"arousal\": row.get(\"arousal\"),\n",
    "            \"user\": row.get(\"user\"),\n",
    "            # Store bbox (relative coords) for traceability\n",
    "            \"bbox_xmin\": float(bb.xmin),\n",
    "            \"bbox_ymin\": float(bb.ymin),\n",
    "            \"bbox_w\": float(bb.width),\n",
    "            \"bbox_h\": float(bb.height),\n",
    "        }\n",
    "    )\n",
    "\n",
    "face_df = pd.DataFrame.from_records(records)\n",
    "if not face_df.empty:\n",
    "    face_df.to_csv(FACE_CSV, index=False)\n",
    "face_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "281f96c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crop_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>run</th>\n",
       "      <th>emotion</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>user</th>\n",
       "      <th>bbox_xmin</th>\n",
       "      <th>bbox_ymin</th>\n",
       "      <th>bbox_w</th>\n",
       "      <th>bbox_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/face_crops/Run_2/Frustrated forty-some...</td>\n",
       "      <td>/Run_2/Frustrated forty-something office/team-...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Interest</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5fd97d5b40332e276ea58209</td>\n",
       "      <td>0.487954</td>\n",
       "      <td>0.197185</td>\n",
       "      <td>0.209047</td>\n",
       "      <td>0.282760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/face_crops/Run_2/Remorseful toddlers c...</td>\n",
       "      <td>/Run_2/Remorseful toddlers court of law/dcfs-c...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Interest</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5985f6bdeef500000111db98</td>\n",
       "      <td>0.698446</td>\n",
       "      <td>0.256576</td>\n",
       "      <td>0.198112</td>\n",
       "      <td>0.297020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/face_crops/Run_2/Scared adolescents pr...</td>\n",
       "      <td>/Run_2/Scared adolescents prison/15-hampton-ro...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Anger</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5fd97d5b40332e276ea58209</td>\n",
       "      <td>0.621183</td>\n",
       "      <td>0.261322</td>\n",
       "      <td>0.193928</td>\n",
       "      <td>0.344931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/face_crops/Run_2/Cheerful soldiers des...</td>\n",
       "      <td>/Run_2/Cheerful soldiers desert/obamaslides_01...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Joy</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5985f6bdeef500000111db98</td>\n",
       "      <td>0.668780</td>\n",
       "      <td>0.132012</td>\n",
       "      <td>0.195301</td>\n",
       "      <td>0.282740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/face_crops/Run_2/Raging elderly war/vi...</td>\n",
       "      <td>/Run_2/Raging elderly war/vietvetwel0010.jpg</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Joy</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5985f6bdeef500000111db98</td>\n",
       "      <td>0.099873</td>\n",
       "      <td>0.280671</td>\n",
       "      <td>0.162464</td>\n",
       "      <td>0.230036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7675</th>\n",
       "      <td>../data/face_crops/Run_2/Guilt teenagers festi...</td>\n",
       "      <td>/Run_2/Guilt teenagers festival/SWUMXL4FUFEUFG...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Vigilance</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>6522a42f39b5bd8f96735aa9</td>\n",
       "      <td>0.072660</td>\n",
       "      <td>0.331239</td>\n",
       "      <td>0.262516</td>\n",
       "      <td>0.466683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7676</th>\n",
       "      <td>../data/face_crops/Run_2/Ashamed seniors rally...</td>\n",
       "      <td>/Run_2/Ashamed seniors rally/ap-19162542641934...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Boredom</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>6522a42f39b5bd8f96735aa9</td>\n",
       "      <td>0.374778</td>\n",
       "      <td>0.174922</td>\n",
       "      <td>0.233983</td>\n",
       "      <td>0.445683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7677</th>\n",
       "      <td>../data/face_crops/Run_2/Peaceful elderly rall...</td>\n",
       "      <td>/Run_2/Peaceful elderly rally/1569185152_chatt...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Fear</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>6522a42f39b5bd8f96735aa9</td>\n",
       "      <td>0.208560</td>\n",
       "      <td>0.235722</td>\n",
       "      <td>0.312466</td>\n",
       "      <td>0.387731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>../data/face_crops/Run_2/Guilt soldiers party/...</td>\n",
       "      <td>/Run_2/Guilt soldiers party/naroda-patiya-mass...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>-2</td>\n",
       "      <td>5</td>\n",
       "      <td>6522a42f39b5bd8f96735aa9</td>\n",
       "      <td>0.316485</td>\n",
       "      <td>0.262779</td>\n",
       "      <td>0.160606</td>\n",
       "      <td>0.285523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7679</th>\n",
       "      <td>../data/face_crops/Run_2/Kind soldiers prison/...</td>\n",
       "      <td>/Run_2/Kind soldiers prison/0_10e0c1_ca4ce5e4_...</td>\n",
       "      <td>Run_2</td>\n",
       "      <td>Admiration</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6522a42f39b5bd8f96735aa9</td>\n",
       "      <td>0.194362</td>\n",
       "      <td>0.253044</td>\n",
       "      <td>0.164702</td>\n",
       "      <td>0.248607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7680 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              crop_path  \\\n",
       "0     ../data/face_crops/Run_2/Frustrated forty-some...   \n",
       "1     ../data/face_crops/Run_2/Remorseful toddlers c...   \n",
       "2     ../data/face_crops/Run_2/Scared adolescents pr...   \n",
       "3     ../data/face_crops/Run_2/Cheerful soldiers des...   \n",
       "4     ../data/face_crops/Run_2/Raging elderly war/vi...   \n",
       "...                                                 ...   \n",
       "7675  ../data/face_crops/Run_2/Guilt teenagers festi...   \n",
       "7676  ../data/face_crops/Run_2/Ashamed seniors rally...   \n",
       "7677  ../data/face_crops/Run_2/Peaceful elderly rall...   \n",
       "7678  ../data/face_crops/Run_2/Guilt soldiers party/...   \n",
       "7679  ../data/face_crops/Run_2/Kind soldiers prison/...   \n",
       "\n",
       "                                             image_path    run     emotion  \\\n",
       "0     /Run_2/Frustrated forty-something office/team-...  Run_2    Interest   \n",
       "1     /Run_2/Remorseful toddlers court of law/dcfs-c...  Run_2    Interest   \n",
       "2     /Run_2/Scared adolescents prison/15-hampton-ro...  Run_2       Anger   \n",
       "3     /Run_2/Cheerful soldiers desert/obamaslides_01...  Run_2         Joy   \n",
       "4          /Run_2/Raging elderly war/vietvetwel0010.jpg  Run_2         Joy   \n",
       "...                                                 ...    ...         ...   \n",
       "7675  /Run_2/Guilt teenagers festival/SWUMXL4FUFEUFG...  Run_2   Vigilance   \n",
       "7676  /Run_2/Ashamed seniors rally/ap-19162542641934...  Run_2     Boredom   \n",
       "7677  /Run_2/Peaceful elderly rally/1569185152_chatt...  Run_2        Fear   \n",
       "7678  /Run_2/Guilt soldiers party/naroda-patiya-mass...  Run_2     Sadness   \n",
       "7679  /Run_2/Kind soldiers prison/0_10e0c1_ca4ce5e4_...  Run_2  Admiration   \n",
       "\n",
       "      valence  arousal                      user  bbox_xmin  bbox_ymin  \\\n",
       "0           0        2  5fd97d5b40332e276ea58209   0.487954   0.197185   \n",
       "1           1        2  5985f6bdeef500000111db98   0.698446   0.256576   \n",
       "2           0        3  5fd97d5b40332e276ea58209   0.621183   0.261322   \n",
       "3           2        4  5985f6bdeef500000111db98   0.668780   0.132012   \n",
       "4           3        4  5985f6bdeef500000111db98   0.099873   0.280671   \n",
       "...       ...      ...                       ...        ...        ...   \n",
       "7675       -1        3  6522a42f39b5bd8f96735aa9   0.072660   0.331239   \n",
       "7676       -1        1  6522a42f39b5bd8f96735aa9   0.374778   0.174922   \n",
       "7677       -1        4  6522a42f39b5bd8f96735aa9   0.208560   0.235722   \n",
       "7678       -2        5  6522a42f39b5bd8f96735aa9   0.316485   0.262779   \n",
       "7679        0        1  6522a42f39b5bd8f96735aa9   0.194362   0.253044   \n",
       "\n",
       "        bbox_w    bbox_h  \n",
       "0     0.209047  0.282760  \n",
       "1     0.198112  0.297020  \n",
       "2     0.193928  0.344931  \n",
       "3     0.195301  0.282740  \n",
       "4     0.162464  0.230036  \n",
       "...        ...       ...  \n",
       "7675  0.262516  0.466683  \n",
       "7676  0.233983  0.445683  \n",
       "7677  0.312466  0.387731  \n",
       "7678  0.160606  0.285523  \n",
       "7679  0.164702  0.248607  \n",
       "\n",
       "[7680 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val/Test split (by emotion)\n",
    "Creates a split column to keep experiments reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_splits(face_df: pd.DataFrame, seed: int = 42):\n",
    "    df = face_df.copy().reset_index(drop=True)\n",
    "    # Use emotion as strata; group by original image_path to avoid leakage\n",
    "    y = df[\"emotion\"].fillna(\"Unknown\")\n",
    "    groups = df[\"image_path\"]\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_idx = np.zeros(len(df), dtype=int)\n",
    "    # First fold as test, second as val, rest train\n",
    "    folds = list(sgkf.split(np.zeros(len(df)), y, groups))\n",
    "    test_idx = folds[0][1]\n",
    "    val_idx = folds[1][1]\n",
    "    split = np.array([\"train\"] * len(df), dtype=object)\n",
    "    split[test_idx] = \"test\"\n",
    "    split[val_idx] = \"val\"\n",
    "    df[\"split\"] = split\n",
    "    return df\n",
    "\n",
    "\n",
    "if not face_df.empty:\n",
    "    face_df = add_splits(face_df)\n",
    "    face_df.to_csv(FACE_CSV, index=False)\n",
    "face_df[\"split\"].value_counts(dropna=False) if not face_df.empty else \"No crops created\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model (ResNet18 backbone with V/A heads)\n",
    "Lightweight head with dropout; supports Monte Carlo Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class FaceEmotionRegressor(nn.Module):\n",
    "    def __init__(self, dropout_rate: float = 0.3):\n",
    "        super().__init__()\n",
    "        backbone = torch.hub.load(\"pytorch/vision:v0.10.0\", \"resnet18\", pretrained=True)\n",
    "        for p in list(backbone.parameters())[:-10]:\n",
    "            p.requires_grad = False\n",
    "        num_features = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.valence_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 128), nn.ReLU(), self.dropout, nn.Linear(128, 1)\n",
    "        )\n",
    "        self.arousal_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 128), nn.ReLU(), self.dropout, nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, n_samples: int = 1):\n",
    "        if n_samples and n_samples > 1:\n",
    "            return self._mc_forward(x, n_samples)\n",
    "        feats = self.backbone(x)\n",
    "        feats = self.dropout(feats)\n",
    "        v = self.valence_head(feats).squeeze(-1)\n",
    "        a = self.arousal_head(feats).squeeze(-1)\n",
    "        return v, a\n",
    "\n",
    "    def _mc_forward(self, x, n_samples: int):\n",
    "        preds = []\n",
    "        self.train()  # enable dropout\n",
    "        for _ in range(n_samples):\n",
    "            v, a = self.forward(x, n_samples=1)\n",
    "            preds.append(torch.stack([v, a]))\n",
    "        self.eval()\n",
    "        preds = torch.stack(preds)\n",
    "        mean = preds.mean(dim=0)\n",
    "        var = preds.var(dim=0)\n",
    "        return mean, var\n",
    "\n",
    "\n",
    "# Basic dataset for crops\n",
    "class FaceCropsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file: str, split: str = \"train\"):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        if \"split\" in self.df.columns:\n",
    "            self.df = self.df[self.df[\"split\"] == split].reset_index(drop=True)\n",
    "        self.tx = T.Compose(\n",
    "            [\n",
    "                T.Resize((224, 224)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row[\"crop_path\"]).convert(\"RGB\")\n",
    "        x = self.tx(img)\n",
    "        v = torch.tensor(row[\"valence\"], dtype=torch.float32)\n",
    "        a = torch.tensor(row[\"arousal\"], dtype=torch.float32)\n",
    "        return x, v, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sketch\n",
    "Quick example loop (not executed by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop (set RUN_TRAINING=True to run)\n",
    "RUN_TRAINING = False\n",
    "if RUN_TRAINING and FACE_CSV.exists():\n",
    "    train_ds = FaceCropsDataset(str(FACE_CSV), split=\"train\")\n",
    "    val_ds = FaceCropsDataset(str(FACE_CSV), split=\"val\")\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=32, shuffle=True, num_workers=2\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_ds, batch_size=32, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FaceEmotionRegressor().to(device)\n",
    "    opt = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3\n",
    "    )\n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.5)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for x, v, a in train_loader:\n",
    "            x, v, a = x.to(device), v.to(device), a.to(device)\n",
    "            opt.zero_grad()\n",
    "            pv, pa = model(x)\n",
    "            loss = loss_fn(pv, v) + loss_fn(pa, a)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += float(loss.item())\n",
    "        print(f\"Epoch {epoch + 1}: train loss {total / max(1, len(train_loader)):.4f}\")\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        vtotal = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, v, a in val_loader:\n",
    "                x, v, a = x.to(device), v.to(device), a.to(device)\n",
    "                pv, pa = model(x)\n",
    "                vtotal += float((loss_fn(pv, v) + loss_fn(pa, a)).item())\n",
    "        print(f\"          val loss {vtotal / max(1, len(val_loader)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset format summary (face_annotations.csv)\n",
    "Each row corresponds to one detected primary face crop. Suggested columns:\n",
    "- `crop_path` (str): absolute/relative path to saved 224×224 crop\n",
    "- `image_path` (str): original relative path from FindingEmo (starts with `/Run_*`)\n",
    "- `run` (str): `Run_1` or `Run_2`\n",
    "- `emotion` (str): discrete emotion label from annotations\n",
    "- `valence` (int/float): V score (dataset provides ints like -3..+3)\n",
    "- `arousal` (int/float): A score (0..6)\n",
    "- `user` (str): annotator id\n",
    "- `bbox_xmin`, `bbox_ymin`, `bbox_w`, `bbox_h` (float, relative 0..1): MediaPipe bbox used for crop\n",
    "- `split` (str): `train`/`val`/`test` after splitting\n",
    "\n",
    "This format is sufficient to train the face emotion regressor with standard PyTorch datasets/dataloaders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
